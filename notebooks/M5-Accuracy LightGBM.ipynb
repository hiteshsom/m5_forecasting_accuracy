{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5-Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspiration\n",
    "\n",
    "* https://www.kaggle.com/ajinomoto132/reduce-mem\n",
    "* https://www.kaggle.com/zmnako/lgbm-update-0-85632\n",
    "* https://www.kaggle.com/ratan123/m5-forecasting-lightgbm-with-timeseries-splits\n",
    "* https://www.kaggle.com/ragnar123/very-fst-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "dir_path = 'C:\\\\Users\\\\Hitesh Somani\\\\Documents\\\\kaggle\\\\m5_forecasting_accuracy' # Please put path of your directory here\n",
    "sys.path.append(f'{dir_path}\\\\src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_function as hf\n",
    "import m5_splitter as m5s\n",
    "import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Please put path of the directory where data us store\n",
    "INPUT = f'{dir_path}\\\\data\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.22 MB\n",
      "Decreased by -5.7%\n",
      "(1969, 14)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- CALENDAR ------------------------ #\n",
    "df_calendar = pd.read_csv(filepath_or_buffer=f'{INPUT}calendar.csv')\n",
    "df_calendar = hf.reduce_mem_usage(df_calendar)\n",
    "print(df_calendar.shape)\n",
    "df_calendar['date'] = pd.to_datetime(df_calendar['date'])\n",
    "df_calendar['weekday'] = df_calendar['weekday'].astype(str)\n",
    "df_calendar['d'] = df_calendar['d'].astype(str)\n",
    "df_calendar['event_name_1'] = df_calendar['event_name_1'].astype(str)\n",
    "df_calendar['event_type_1'] = df_calendar['event_type_1'].astype(str)\n",
    "df_calendar['event_name_2'] = df_calendar['event_name_2'].astype(str)\n",
    "df_calendar['event_type_2'] = df_calendar['event_type_2'].astype(str)\n",
    "\n",
    "df_calendar['event_tomorrow_1'] = df_calendar['event_name_1'].shift(-1)\n",
    "df_calendar['event_tomorrow_2'] = df_calendar['event_name_2'].shift(-1)\n",
    "df_calendar['event_type_tomorrow_1'] = df_calendar['event_type_1'].shift(-1)\n",
    "df_calendar['event_type_tomorrow_2'] = df_calendar['event_type_2'].shift(-1)\n",
    "\n",
    "df_calendar = df_calendar.fillna(value='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.12 MB\n",
      "Memory usage after optimization is: 0.15 MB\n",
      "Decreased by -19.7%\n"
     ]
    }
   ],
   "source": [
    "# event_name_1 and event_name_2 should be fitted together since both are essentailly the same\n",
    "le1 = LabelEncoder()\n",
    "le1.fit(pd.concat(objs=[df_calendar['event_name_1'], df_calendar['event_name_2']], axis=0))\n",
    "df_calendar['event_name_1'] = le1.transform(df_calendar['event_name_1'])\n",
    "df_calendar['event_name_2'] = le1.transform(df_calendar['event_name_2'])\n",
    "\n",
    "# event_type_1 and event_type_2 should be fitted together since both are essentailly the same\n",
    "le2 = LabelEncoder()\n",
    "le2.fit(pd.concat(objs=[df_calendar['event_type_1'], df_calendar['event_type_2']], axis=0))\n",
    "df_calendar['event_type_1'] = le2.transform(df_calendar['event_type_1'])\n",
    "df_calendar['event_type_2'] = le2.transform(df_calendar['event_type_2'])\n",
    "\n",
    "le3 = LabelEncoder()\n",
    "le3.fit(pd.concat(objs=[df_calendar['event_tomorrow_1'], df_calendar['event_tomorrow_2']], axis=0))\n",
    "df_calendar['event_tomorrow_1'] = le3.transform(df_calendar['event_tomorrow_1'])\n",
    "df_calendar['event_tomorrow_2'] = le3.transform(df_calendar['event_tomorrow_2'])\n",
    "\n",
    "le4 = LabelEncoder()\n",
    "le4.fit(pd.concat(objs=[df_calendar['event_type_tomorrow_1'], df_calendar['event_type_tomorrow_2']], axis=0))\n",
    "df_calendar['event_type_tomorrow_1'] = le4.transform(df_calendar['event_type_tomorrow_1'])\n",
    "df_calendar['event_type_tomorrow_2'] = le4.transform(df_calendar['event_type_tomorrow_2'])\n",
    "\n",
    "\n",
    "df_calendar = hf.reduce_mem_usage(df_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>event_tomorrow_1</th>\n",
       "      <th>event_tomorrow_2</th>\n",
       "      <th>event_type_tomorrow_1</th>\n",
       "      <th>event_type_tomorrow_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1965</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1966</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>Friday</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1967</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1968</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1969</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  wm_yr_wk    weekday  wday  month  year       d  event_name_1  \\\n",
       "1964 2016-06-15     11620  Wednesday     5      6  2016  d_1965            30   \n",
       "1965 2016-06-16     11620   Thursday     6      6  2016  d_1966            30   \n",
       "1966 2016-06-17     11620     Friday     7      6  2016  d_1967            30   \n",
       "1967 2016-06-18     11621   Saturday     1      6  2016  d_1968            30   \n",
       "1968 2016-06-19     11621     Sunday     2      6  2016  d_1969            16   \n",
       "\n",
       "      event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \\\n",
       "1964             4            30             4        0        1        1   \n",
       "1965             4            30             4        0        0        0   \n",
       "1966             4            30             4        0        0        0   \n",
       "1967             4            30             4        0        0        0   \n",
       "1968             3             7             0        0        0        0   \n",
       "\n",
       "      event_tomorrow_1  event_tomorrow_2  event_type_tomorrow_1  \\\n",
       "1964                30                30                      4   \n",
       "1965                30                30                      4   \n",
       "1966                30                30                      4   \n",
       "1967                16                 7                      3   \n",
       "1968                30                30                      4   \n",
       "\n",
       "      event_type_tomorrow_2  \n",
       "1964                      4  \n",
       "1965                      4  \n",
       "1966                      4  \n",
       "1967                      0  \n",
       "1968                      4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calendar.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- SELLING PRICES ------------------------ #\n",
    "df_selling_prices = pd.read_csv(f'{INPUT}sell_prices.csv')\n",
    "print(df_selling_prices.shape)\n",
    "df_selling_prices['store_id'] = df_selling_prices['store_id'].astype(str)\n",
    "df_selling_prices['item_id'] = df_selling_prices['item_id'].astype(str)\n",
    "df_selling_prices['sell_price_cents'] = df_selling_prices['sell_price'] - df_selling_prices['sell_price'].astype(int) \n",
    "df_selling_prices['sell_price_preceived'] = df_selling_prices['sell_price'].astype(int)\n",
    "df_selling_prices = hf.reduce_mem_usage(df_selling_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cols(factor=0.5):\n",
    "    cols =['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    start = int(1914*(1-factor))\n",
    "    days = ['d_' + str(i) for i in range(start, 1914, 1)]\n",
    "    cols += days\n",
    "    return cols\n",
    "\n",
    "columns = generate_cols(factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- SALES TRAIN VALIDATION ---------------------- #\n",
    "df_sales_train_val = pd.read_csv(f'{INPUT}sales_train_validation.csv', usecols=columns)\n",
    "df_sales_train_val['id'] = df_sales_train_val['id'].astype(str)\n",
    "df_sales_train_val['item_id'] = df_sales_train_val['item_id'].astype(str)\n",
    "df_sales_train_val['store_id'] = df_sales_train_val['store_id'].astype(str)\n",
    "df_sales_train_val['dept_id'] = df_sales_train_val['dept_id'].astype(str)\n",
    "df_sales_train_val['cat_id'] = df_sales_train_val['cat_id'].astype(str)\n",
    "df_sales_train_val['state_id'] = df_sales_train_val['state_id'].astype(str)\n",
    "\n",
    "print(f'Shape before melting: {df_sales_train_val.shape}')\n",
    "print(f\"Unique products before melting: {df_sales_train_val['id'].nunique()}\")\n",
    "\n",
    "df_sales_train_val = pd.melt(df_sales_train_val, id_vars=['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id'], var_name='d',\n",
    "                              value_name='demand')\n",
    "\n",
    "print(f'\\nShape after melting: {df_sales_train_val.shape}')\n",
    "print(f\"Unique products after melting: {df_sales_train_val['id'].nunique()}\")\n",
    "\n",
    "# 0 means validation set\n",
    "df_sales_train_val['set'] = 0\n",
    "df_sales_train_val['strategy'] = 'train'\n",
    "df_sales_train_val['id'] = df_sales_train_val['item_id'].str.cat(df_sales_train_val['store_id'], sep='_')\n",
    "\n",
    "\n",
    "print(f'\\nShape after making id from item_id and store_id: {df_sales_train_val.shape}')\n",
    "print(f\"Unique products after id from item_id and store_id: {df_sales_train_val['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_train_val = df_sales_train_val.drop_duplicates()\n",
    "print(f'Shape after making id from item_id and store_id: {df_sales_train_val.shape}')\n",
    "print(f\"Unique products after id from item_id and store_id: {df_sales_train_val['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_train_val = hf.reduce_mem_usage(df_sales_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- ADDING CALENDAR AND SELLING PRICES to SALES TRAIN VAL ---------------------- #\n",
    "df_sales_train_val = df_sales_train_val.merge(df_calendar, on = ['d'], how='left')\n",
    "df_sales_train_val = df_sales_train_val.merge(df_selling_prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape after merging: {df_sales_train_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_train_val = df_sales_train_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = []\n",
    "for num, col in enumerate(['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']):\n",
    "    le_id = LabelEncoder()\n",
    "    le.append(le_id)\n",
    "    le[num].fit(df_sales_train_val[col])\n",
    "    df_sales_train_val[f'{col}_encoded'] = le[num].transform(df_sales_train_val[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- SAMPLE SUBMISSION -------------------------- #\n",
    "df_sample_submission = pd.read_csv(f'{INPUT}sample_submission.csv')\n",
    "df_sample_submission = hf.reduce_mem_usage(df_sample_submission)\n",
    "df_sample_submission['id'] = df_sample_submission['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_submission['state_id'] = df_sample_submission['id'].str[-15:-13]\n",
    "df_sample_submission['store_id'] = df_sample_submission['id'].str[-15:-11]\n",
    "df_sample_submission['item_id'] = df_sample_submission['id'].str[0:-16]\n",
    "df_sample_submission['dept_id'] = df_sample_submission['id'].str[0:-20]\n",
    "df_sample_submission['cat_id'] = df_sample_submission['id'].str[0:-22]\n",
    "df_sample_submission.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- SALES TEST -------------------------- #\n",
    "df_sales_test = pd.melt(df_sample_submission, id_vars=['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id'], var_name='d', value_name='demand')\n",
    "\n",
    "df_sales_test['d2'] = df_sales_test['d'].str[1:]\n",
    "df_sales_test['d2'] = df_sales_test['d2'].astype(int)\n",
    "df_sales_test.loc[df_sales_test['id'].str[-10:]=='evaluation', 'd2'] = df_sales_test.loc[df_sales_test['id'].str[-10:]=='evaluation', 'd2'] + 28\n",
    "df_sales_test['d2'] = df_sales_test['d2'] + 1913\n",
    "df_sales_test['d2'] = df_sales_test['d2'].astype(str)\n",
    "df_sales_test['d2'] = 'd_' + df_sales_test['d2']\n",
    "\n",
    "# 1 indicates test set\n",
    "df_sales_test['set'] = 1\n",
    "df_sales_test.loc[df_sales_test['id'].str[-10:]=='validation', 'strategy'] = 'validation'\n",
    "df_sales_test.loc[df_sales_test['id'].str[-10:]=='evaluation', 'strategy'] = 'evaluation'\n",
    "df_sales_test['original_id'] = df_sales_test['id']\n",
    "df_sales_test['id'] = df_sales_test['id'].str[0:-11]\n",
    "\n",
    "df_sales_test = df_sales_test.drop(columns=['d'])\n",
    "df_sales_test = df_sales_test.rename(columns={'d2': 'd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------- ADDING CALENDAR AND SELLING PRICES to SALES TRAIN VAL ---------------------- #\n",
    "df_sales_test = df_sales_test.merge(df_calendar, on = ['d'], how='left')\n",
    "df_sales_test = df_sales_test.merge(df_selling_prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Making {id}_encoded features ------------------------ #\n",
    "for num, col in enumerate(['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']):\n",
    "    df_sales_test[f'{col}_encoded'] = le[num].transform(df_sales_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- TRAIN VAL TEST -------------------------- #\n",
    "df_sales_test = df_sales_test.drop(columns=['original_id'])\n",
    "df_train_val_test = pd.concat(objs=[df_sales_train_val, df_sales_test], axis=0, ignore_index=True)\n",
    "df_train_val_test = hf.reduce_mem_usage(df_train_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test['d_number'] = df_train_val_test['d'].str[2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test = df_train_val_test.sort_values(by=['id', 'd_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test = df_train_val_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test = fe.simple_fe_extra(df_train_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train_val_test.columns:\n",
    "    if is_numeric_dtype(df_train_val_test[col]):\n",
    "        df_train_val_test[col] = df_train_val_test[col].round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test['is_weekend'] = 0\n",
    "df_train_val_test.loc[df_train_val_test['weekday'].isin(['Saturday', 'Sunday']), 'is_weekend'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_val_test[\"price_rank_store_item\"] = df_train_val_test.groupby(by = [\"store_id\", \"item_id\"])['sell_price'].rank(method='dense',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a feature 'prob_high_demand' which is a float which is max on day, day before and day after the event_name_1 \n",
    "# and which decreases as we move away from event date on both in past and future. Think of it as a bell curve over date\n",
    "# peaking at event date\n",
    "\n",
    "indexes = df_train_val_test.groupby(by=['event_name_1']).groups\n",
    "del indexes[30]                # 30 representes 'nan' event\n",
    "high_demand_indexes = []\n",
    "mid_demand_indexes = []\n",
    "mid_low_demand_indexes = []\n",
    "for key, val in indexes.items():\n",
    "    for value in indexes[key]:\n",
    "        high_demand_indexes.append(value)\n",
    "        high_demand_indexes.append(value + 1)\n",
    "        mid_demand_indexes.append(value + 2)\n",
    "        mid_low_demand_indexes.append(value + 3)\n",
    "        high_demand_indexes.append(value - 1)\n",
    "        mid_demand_indexes.append(value - 2)\n",
    "        mid_low_demand_indexes.append(value - 3)\n",
    "        \n",
    "high_demand_indexes = list(set(high_demand_indexes))\n",
    "mid_demand_indexes = list(set(mid_demand_indexes))\n",
    "mid_low_demand_indexes = list(set(mid_low_demand_indexes))\n",
    "\n",
    "df_train_val_test['prob_high_demand'] = 0\n",
    "high_demand_indexes = list(set(df_train_val_test.index).intersection(set(high_demand_indexes)))\n",
    "mid_demand_indexes = list(set(df_train_val_test.index).intersection(set(mid_demand_indexes)))\n",
    "mid_low_demand_indexes = list(set(df_train_val_test.index).intersection(set(mid_low_demand_indexes)))\n",
    "\n",
    "df_train_val_test.loc[mid_low_demand_indexes, 'prob_high_demand'] = 0.3\n",
    "df_train_val_test.loc[mid_demand_indexes, 'prob_high_demand'] = 0.6\n",
    "df_train_val_test.loc[high_demand_indexes, 'prob_high_demand'] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test['is_weekend'] = df_train_val_test['is_weekend'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tomorrow_some_event is causing over fitting \n",
    "# df_train_val_test = df_train_val_test.drop(columns=['event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_val_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_val_test['diff_high_low_demand'] = np.nan\n",
    "# high_low = prep.HighLow()\n",
    "# high_low.train(df_train_val_test.loc[df_train_val_test['strategy']=='train', :])\n",
    "# df_train_val_test = high_low.apply(df_train_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_val_test['diff_high_low_demand'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_train_val_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['event_tomorrow_1', \n",
    "            'event_tomorrow_2','event_type_tomorrow_1','event_type_tomorrow_2']:\n",
    "    cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_val_test.loc[df_train_val_test['set']==0, cols]\n",
    "y_train = X_train['demand']\n",
    "X_train = X_train.drop(columns=['d_number', 'set', 'demand', 'strategy', 'week', 'day', 'dayofweek',\n",
    "                                'wday','month']) # drop 'set' bcoz it can cause data leak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "m5split2 = m5s.M5Split2(n_splits=0, group_id='id', date_col='date', train_size=365, gap_size=0, val_size=28, step=28)\n",
    "fold = 0\n",
    "estimators = []\n",
    "for trn_idx, _, val_idx in m5split2.split(X_train):\n",
    "    fold_s_time = time.time()\n",
    "    s_time = time.time()\n",
    "    X_trn = X_train.loc[trn_idx, :]\n",
    "    y_trn = y_train[trn_idx]\n",
    "    X_val = X_train.loc[val_idx, :]\n",
    "    y_val = y_train[val_idx]\n",
    "    \n",
    "    e_time = time.time()\n",
    "    print(f'Time taken for splitting of fold{fold}: {e_time-s_time} seconds')\n",
    "\n",
    "    s_time = time.time()\n",
    "    lgbmr = lightgbm.LGBMRegressor(objective = \"poisson\",\n",
    "                                   boosting_type = 'gbdt',\n",
    "                                   n_estimators = 500,\n",
    "                                   random_state = 20,\n",
    "                                   learning_rate = 0.1,\n",
    "                                   bagging_fraction = 0.75,\n",
    "                                   bagging_freq = 10, \n",
    "                                   colsample_bytree = 0.75)\n",
    "    lgbmr.fit(X_trn.drop(columns=['date', 'd']), y_trn)\n",
    "    estimators.append(lgbmr)\n",
    "    y_predict = np.around(lgbmr.predict(X_val.drop(columns=['date', 'd'])))\n",
    "    e_time = time.time()\n",
    "    print(f'Time taken for learning of fold{fold}: {e_time-s_time} seconds')\n",
    "    \n",
    "    s_time = time.time()\n",
    "    rmse_score = np.sqrt(mse(y_val, y_predict))\n",
    "    print(f'CV RMSE score of fold{fold} (train_len: {len(trn_idx)}, val_len: {len(val_idx)}): {rmse_score}')\n",
    "    scores.append(rmse_score)\n",
    "    fold_e_time = time.time()\n",
    "    print(f'Total time taken for fold{fold}: {fold_e_time-fold_s_time} seconds\\n')\n",
    "    fold += 1\n",
    "\n",
    "print(f'Mean score: {sum(scores)/len(scores)}, Stdev: {np.std(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = X_train.loc[X_train['date']>='2015-04-24 00:00:00']\n",
    "y_trn = y_train[X_trn.index]\n",
    "lgbmr = lightgbm.LGBMRegressor(objective = \"poisson\",\n",
    "                                   boosting_type = 'gbdt',\n",
    "                                   n_estimators = 500,\n",
    "                                   random_state = 20,\n",
    "                                   learning_rate = 0.1,\n",
    "                                   bagging_fraction = 0.75,\n",
    "                                   bagging_freq = 10, \n",
    "                                   colsample_bytree = 0.75)\n",
    "lgbmr.fit(X_trn.drop(columns=['date', 'd']), y_trn)\n",
    "estimators.append(lgbmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_train_val_test.loc[df_train_val_test['set']==1, cols]\n",
    "# X_test = X_test.drop(columns=['date', 'd_number', 'set', 'demand', 'strategy', 'rolling_5'])\n",
    "X_test = X_test.drop(columns=['d_number', 'set', 'demand', 'strategy', 'week', 'day', 'dayofweek', \n",
    "                              'wday', 'month'])\n",
    "predictions = pd.DataFrame(columns=[estimator.__class__.__name__ + str(estimators.index(estimator)) for estimator in estimators])\n",
    "\n",
    "for i, estimator in enumerate(estimators):\n",
    "    y_predict = estimator.predict(X_test.drop(columns=['date', 'd']))\n",
    "    predictions[predictions.columns[i]] = pd.Series(y_predict)\n",
    "\n",
    "cols = list(predictions.columns)\n",
    "predictions['final'] = np.mean(predictions.loc[:, cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_col = 'LGBMRegressor0'\n",
    "predictions.loc[predictions[prediction_col] < 0.0, prediction_col] = 0.0 \n",
    "\n",
    "predictions = predictions.set_index(X_test.index)\n",
    "\n",
    "X_test['pred'] = predictions['LGBMRegressor0']\n",
    "\n",
    "\n",
    "df_train_val_test.loc[X_test.index, 'demand'] = X_test['pred']\n",
    "df_submit = df_train_val_test.loc[X_test.index, :]\n",
    "df_submit['original_id'] = df_submit['id'].str.cat(df_submit['strategy'], sep='_')\n",
    "\n",
    "df_submit.to_csv(f'{INPUT}submit.csv', index=False)\n",
    "\n",
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv(f'{INPUT}submit.csv')\n",
    "df_submit = hf.reduce_mem_usage(df_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit['d2'] = df_submit['d'].str[2:]\n",
    "df_submit['d2'] = df_submit['d2'].astype(int)\n",
    "df_submit.loc[df_submit['strategy']=='validation', 'd2'] = df_submit.loc[df_submit['strategy']=='validation', 'd2'] - 1913\n",
    "df_submit.loc[df_submit['strategy']=='evaluation', 'd2'] = df_submit.loc[df_submit['strategy']=='evaluation', 'd2'] - 1941\n",
    "df_submit['d2'] = df_submit['d2'].astype(str)\n",
    "df_submit['d2'] = 'F' + df_submit['d2']\n",
    "df_submit.loc[df_submit['strategy']=='evaluation', 'demand'] = 0.0\n",
    "df_submission = df_submit.loc[:, ['original_id', 'demand', 'd2']].pivot(index='original_id', columns='d2', values='demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_order = ['F' + str(i) for i in range(1,29)]\n",
    "df_submission = df_submission[cols_order]\n",
    "\n",
    "df_sample_submission = pd.read_csv(f'{INPUT}sample_submission.csv')\n",
    "df_sample_submission = df_sample_submission.set_index(keys='id')\n",
    "df_submission = df_submission.reindex(df_sample_submission.index)\n",
    "df_submission = df_submission.reset_index()\n",
    "df_submission = df_submission.rename(columns={'original_id': 'id'})\n",
    "df_submission.to_csv(f'{INPUT}submission.csv', index=False)\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m48"
  },
  "kernelspec": {
   "display_name": "m5_forecasting_accuracy",
   "language": "python",
   "name": "m5_forecasting_accuracy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
